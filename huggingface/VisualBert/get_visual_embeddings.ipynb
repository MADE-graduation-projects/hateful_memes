{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyyaml==5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@05bc8439ca10e11300d9d34e4fe0dd1d3f42773a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1667340274848,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "643yOpAZwRWq",
    "outputId": "331eea49-0de7-4e34-c003-fbeab209741e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667340274848,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "jyKj-udtwT8o",
    "outputId": "28435a7c-273c-488e-e8db-b86b82009b05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 194572,
     "status": "ok",
     "timestamp": 1667340469415,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "_P5ZDpfKXlHX"
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667340469892,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "xACCRQgLfhLG"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1667341236409,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "i8EUjamjkT6p"
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "    \n",
    "    return images, batched_inputs\n",
    "\n",
    "\n",
    "def get_features(model, images):\n",
    "    features = model.backbone(images.tensor)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals\n",
    "\n",
    "\n",
    "def get_box_features(model, features, proposals, batch_size):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(batch_size, 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list\n",
    "\n",
    "\n",
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "\n",
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas, proposals):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes\n",
    "\n",
    "\n",
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes\n",
    "\n",
    "\n",
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf\n",
    "\n",
    "\n",
    "\n",
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes\n",
    "\n",
    "\n",
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1667342511962,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "j4Sh6otik73l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1667342513855,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "rJP7gj4bkJao"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667342513856,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vKwPzLa4krkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6761,
     "status": "ok",
     "timestamp": 1667342520614,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "SfPCSBSYme7c"
   },
   "outputs": [],
   "source": [
    "def get_visual_embeddings(cfg, model, paths):\n",
    "    img_arr = []\n",
    "    for path in paths:\n",
    "        img = plt.imread(path)        \n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Detectron expects BGR images\n",
    "        img_arr.append(img_bgr)\n",
    "        \n",
    "        \n",
    "    images, batched_inputs = prepare_image_inputs(cfg, img_arr)\n",
    "    features = get_features(model, images)\n",
    "    proposals = get_proposals(model, images, features)\n",
    "    box_features, features_list = get_box_features(model, features, proposals, len(images))\n",
    "    pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)\n",
    "\n",
    "    boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas, proposals)\n",
    "\n",
    "    output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "\n",
    "    temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "    keep_boxes, max_conf = [],[]\n",
    "    for keep_box, mx_conf in temp:\n",
    "        keep_boxes.append(keep_box)\n",
    "        max_conf.append(mx_conf)\n",
    "\n",
    "    keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "\n",
    "    visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "    \n",
    "    return visual_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1639,
     "status": "ok",
     "timestamp": 1667342537635,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "WbMs_tnKmu8G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем файл\n",
    "#files.upload()\n",
    "\n",
    "#! mkdir ~/.kaggle\n",
    "#! cp kaggle.json ~/.kaggle/\n",
    "#!chmod 600 /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d parthplc/facebook-hateful-meme-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip facebook-hateful-meme-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1667342582933,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "ujjSh0I5mxVE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1667342583940,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vtSExOO3m4xN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1667342583940,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "VxxAVAbSnBGN"
   },
   "outputs": [],
   "source": [
    "cfg = load_config_and_model_weights(cfg_path)\n",
    "\n",
    "model = get_model(cfg)\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "            \n",
    "    def __getitem__(self, index: int):\n",
    "        #image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"]))   \n",
    "        \n",
    "        path = os.path.join(self.data_dir, self.data[index][\"img\"])\n",
    "        text = self.data[index][\"text\"]\n",
    "        \n",
    "        label = self.data[index][\"label\"]\n",
    "            \n",
    "        return self.data[index][\"img\"], path,  text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0   8291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2   3745  img/03745.png      1   \n",
       "\n",
       "                                                text  \n",
       "0              white people is this a shooting range  \n",
       "1                              bravery at its finest  \n",
       "2  your order comes to $37.50 and your white priv...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/alex/hm/data/'\n",
    "\n",
    "train_path = data_dir + 'train.jsonl'\n",
    "dev_path = data_dir + 'dev.jsonl'\n",
    "\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(dev_path, lines=True)\n",
    "\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667342583941,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "1s3PMb8Asp8d"
   },
   "outputs": [],
   "source": [
    "train_dataset = HatefulMemesDataset(train_path)\n",
    "val_dataset = HatefulMemesDataset(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b592a5150d4ec1bef18baace118a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642881969/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('img/51306.png',) shape '[1, 1000, 1024]' is invalid for input of size 1017856\n",
      "('img/83264.png',) shape '[1, 1000, 1024]' is invalid for input of size 1009664\n"
     ]
    }
   ],
   "source": [
    "dct_visual_embeddings_val = {}\n",
    "\n",
    "for id, paths, texts, labels in tqdm(DataLoader(val_dataset, batch_size=1)):\n",
    "    try:\n",
    "        x = get_visual_embeddings(cfg, model, [paths[0]])\n",
    "        dct_visual_embeddings_val[id[0]] = x\n",
    "    except Exception as ex:\n",
    "        print(id, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('visual_embeddings_val.pkl', 'wb') as f:\n",
    "    pickle.dump(dct_visual_embeddings_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd13d1372d94312bdae424ed5ae4a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('img/47326.png',) shape '[1, 1000, 1024]' is invalid for input of size 986112\n",
      "('img/08654.png',) shape '[1, 1000, 1024]' is invalid for input of size 1015808\n",
      "('img/01896.png',) shape '[1, 1000, 1024]' is invalid for input of size 1016832\n",
      "('img/40217.png',) shape '[1, 1000, 1024]' is invalid for input of size 997376\n",
      "('img/07215.png',) shape '[1, 1000, 1024]' is invalid for input of size 993280\n",
      "('img/97108.png',) shape '[1, 1000, 1024]' is invalid for input of size 1006592\n",
      "('img/03759.png',) shape '[1, 1000, 1024]' is invalid for input of size 1009664\n",
      "('img/51304.png',) shape '[1, 1000, 1024]' is invalid for input of size 1014784\n",
      "('img/04719.png',) shape '[1, 1000, 1024]' is invalid for input of size 1020928\n",
      "('img/82673.png',) shape '[1, 1000, 1024]' is invalid for input of size 1001472\n",
      "('img/70194.png',) shape '[1, 1000, 1024]' is invalid for input of size 987136\n",
      "('img/18490.png',) shape '[1, 1000, 1024]' is invalid for input of size 1018880\n",
      "('img/75319.png',) shape '[1, 1000, 1024]' is invalid for input of size 998400\n",
      "('img/69480.png',) shape '[1, 1000, 1024]' is invalid for input of size 978944\n",
      "('img/74385.png',) shape '[1, 1000, 1024]' is invalid for input of size 995328\n",
      "('img/02674.png',) shape '[1, 1000, 1024]' is invalid for input of size 1010688\n",
      "('img/18469.png',) shape '[1, 1000, 1024]' is invalid for input of size 1016832\n",
      "('img/70395.png',) shape '[1, 1000, 1024]' is invalid for input of size 1013760\n",
      "('img/67438.png',) shape '[1, 1000, 1024]' is invalid for input of size 1017856\n",
      "('img/45819.png',) shape '[1, 1000, 1024]' is invalid for input of size 1000448\n",
      "('img/42073.png',) shape '[1, 1000, 1024]' is invalid for input of size 1009664\n",
      "('img/08571.png',) shape '[1, 1000, 1024]' is invalid for input of size 1002496\n",
      "('img/87934.png',) shape '[1, 1000, 1024]' is invalid for input of size 973824\n",
      "('img/46095.png',) shape '[1, 1000, 1024]' is invalid for input of size 1021952\n",
      "('img/47289.png',) shape '[1, 1000, 1024]' is invalid for input of size 996352\n",
      "('img/21347.png',) shape '[1, 1000, 1024]' is invalid for input of size 1017856\n",
      "('img/48751.png',) shape '[1, 1000, 1024]' is invalid for input of size 1006592\n",
      "('img/50931.png',) shape '[1, 1000, 1024]' is invalid for input of size 1015808\n",
      "('img/71506.png',) shape '[1, 1000, 1024]' is invalid for input of size 1013760\n",
      "('img/82914.png',) shape '[1, 1000, 1024]' is invalid for input of size 1010688\n",
      "('img/95764.png',) shape '[1, 1000, 1024]' is invalid for input of size 988160\n",
      "('img/52614.png',) shape '[1, 1000, 1024]' is invalid for input of size 1008640\n",
      "('img/43791.png',) shape '[1, 1000, 1024]' is invalid for input of size 1005568\n",
      "('img/31485.png',) shape '[1, 1000, 1024]' is invalid for input of size 984064\n",
      "('img/10283.png',) shape '[1, 1000, 1024]' is invalid for input of size 997376\n",
      "('img/17398.png',) shape '[1, 1000, 1024]' is invalid for input of size 1022976\n"
     ]
    }
   ],
   "source": [
    "dct_visual_embeddings_train = {}\n",
    "\n",
    "for id, paths, texts, labels in tqdm(DataLoader(train_dataset, batch_size=1)):\n",
    "    try:\n",
    "        x = get_visual_embeddings(cfg, model, [paths[0]])\n",
    "        dct_visual_embeddings_train[id[0]] = x\n",
    "    except Exception as ex:\n",
    "        print(id, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('visual_embeddings_train.pkl', 'wb') as f:\n",
    "    pickle.dump(dct_visual_embeddings_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667342583941,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "BAjub3VYtiXK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('visual_embeddings_val.pkl', 'rb') as f:\n",
    "    visual_embeddings_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('visual_embeddings_train.pkl', 'rb') as f:\n",
    "    visual_embeddings_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
