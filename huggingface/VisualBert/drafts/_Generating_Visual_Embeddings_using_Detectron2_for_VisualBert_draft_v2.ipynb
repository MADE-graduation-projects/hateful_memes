{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qv-4b7LYOVw"
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSobfsQYTEG"
   },
   "source": [
    "*Disclaimer: This notebook is based on my understanding of the [detectron2](https://github.com/facebookresearch/detectron2) and the [visualbert](https://github.com/uclanlp/visualbert) repositories. Hence, I do not guarantee that this is the \"correct\" or \"recommended\" way to get visual embeddings from detectron2. Having said that, I'm definitely looking to improve this notebook and open to any criticism/suggestions. You can reach me at chhablani.gunjan@gmail.com with any issues that concern you regarding this notebook.*\n",
    "\n",
    "This notebook is based on the concept in the [script to extract image features](https://github.com/uclanlp/visualbert/blob/master/utils/get_image_features/extract_image_features_nlvr.py) for NLVR2 task in the [visualbert](https://github.com/uclanlp/visualbert) repository. You can refer to this script for a \"safer\" way to extract visual embeddings. The script uses [detectron](https://github.com/facebookresearch/Detectron) and it'll be fairly easier to use it (I hope) without getting into the nitty-gritty.\n",
    "\n",
    "However, for the sake of using detectron2, which will have better support (for the foreseeable future) than detectron, I present this notebook example to you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY8GzVcva0Xc"
   },
   "source": [
    "For extracting visual embeddings, we need the features from various regions in the image which are used in the classification. This means that we need to \"detect\" the regions which might have objects in them.\n",
    "\n",
    "The detectron2 library, off-the-shelf, does not support intermediate tensor extraction. But, there are ways the user can get the values of these tensors with some effort. See the docs [here](https://detectron2.readthedocs.io/en/latest/tutorials/models.html#partially-execute-a-model). In this notebook, I will be using the *partial execution* method as described in the docs. I admit that the other approaches might be easier or better suited, but this is just a start ;).\n",
    "\n",
    "**Tip:** If you're looking to play with detectron2, you might like this [Colab tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE_GB-tubtJs"
   },
   "source": [
    "For the purpose of this notebook, I will be using an example from the [VQA v2](https://visualqa.org/download.html) validation set, as it is one of the tasks VisualBert has been used for. VisualBert authors used pre-generated embeddings for VQA v2, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-mLSDb74g58"
   },
   "source": [
    "## How it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_Aa4ojk4mTx"
   },
   "source": [
    "The model checkpoint that we will be using for this notebook is a MaskRCNN+ResNet-101+FPN checkpoint.\n",
    "\n",
    "First, the image features are generated at various scales using the ResNet+FPN backbone. These features are then passed to the region proposal network or RPN. RPN generates 1000 region proposals, which are then passed to ROI Heads. ROI Heads perform the classification and box-regression and after that the predictions are aligned using ROIAlign layer and to the mask RCNN heads.\n",
    "\n",
    "We want to extract the box features in the ROI heads which are used for classification. However, we don't want to select all the proposals (as there are 1000 of them!). For the same, we use the NMS with a threshold. Then the boxes are further filtered using a class score threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf-pOxtSfPF0"
   },
   "source": [
    "### Install Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1667340274848,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "643yOpAZwRWq",
    "outputId": "331eea49-0de7-4e34-c003-fbeab209741e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667340274848,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "jyKj-udtwT8o",
    "outputId": "28435a7c-273c-488e-e8db-b86b82009b05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 194572,
     "status": "ok",
     "timestamp": 1667340469415,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "_P5ZDpfKXlHX"
   },
   "outputs": [],
   "source": [
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "%%capture\n",
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@05bc8439ca10e11300d9d34e4fe0dd1d3f42773a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG9jjTSOfhX6"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1667340469892,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "xACCRQgLfhLG"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, VisualBertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1667341236409,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "i8EUjamjkT6p"
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    #cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "    \n",
    "    return images, batched_inputs\n",
    "\n",
    "\n",
    "def get_features(model, images):\n",
    "    features = model.backbone(images.tensor)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals\n",
    "\n",
    "\n",
    "def get_box_features(model, features, proposals, batch_size):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(batch_size, 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list\n",
    "\n",
    "\n",
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "\n",
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes\n",
    "\n",
    "\n",
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes\n",
    "\n",
    "\n",
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf\n",
    "\n",
    "\n",
    "\n",
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes\n",
    "\n",
    "\n",
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1667342511962,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "j4Sh6otik73l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1667342513855,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "rJP7gj4bkJao"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667342513856,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vKwPzLa4krkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 6761,
     "status": "ok",
     "timestamp": 1667342520614,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "SfPCSBSYme7c"
   },
   "outputs": [],
   "source": [
    "def get_visual_embeddings(cfg, model, paths):\n",
    "    img_arr = []\n",
    "    for path in paths:\n",
    "        img = plt.imread(path)        \n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Detectron expects BGR images\n",
    "        img_arr.append(img_bgr)\n",
    "        \n",
    "        \n",
    "    images, batched_inputs = prepare_image_inputs(cfg, img_arr)\n",
    "    features = get_features(model, images)\n",
    "    proposals = get_proposals(model, images, features)\n",
    "    box_features, features_list = get_box_features(model, features, proposals, len(images))\n",
    "    pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)\n",
    "\n",
    "    boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)\n",
    "\n",
    "    output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "\n",
    "    temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "    keep_boxes, max_conf = [],[]\n",
    "    for keep_box, mx_conf in temp:\n",
    "        keep_boxes.append(keep_box)\n",
    "        max_conf.append(mx_conf)\n",
    "\n",
    "    keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "\n",
    "    visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "    \n",
    "    return visual_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 1639,
     "status": "ok",
     "timestamp": 1667342537635,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "WbMs_tnKmu8G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем файл\n",
    "files.upload()\n",
    "\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d parthplc/facebook-hateful-meme-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip facebook-hateful-meme-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1667342582189,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "H_t89IhBXKVH"
   },
   "outputs": [],
   "source": [
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.transforms = transforms\n",
    "            \n",
    "    def __getitem__(self, index: int):\n",
    "        #image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"]))   \n",
    "        \n",
    "        path = os.path.join(self.data_dir, self.data[index][\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1667342582933,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "ujjSh0I5mxVE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1667342583940,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vtSExOO3m4xN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1667342583940,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "VxxAVAbSnBGN"
   },
   "outputs": [],
   "source": [
    "cfg = load_config_and_model_weights(cfg_path)\n",
    "\n",
    "model = get_model(cfg)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from cv2 import cv2\n",
    "\n",
    "\n",
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "            \n",
    "    def __getitem__(self, index: int):\n",
    "        #image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"]))   \n",
    "        \n",
    "        path = os.path.join(self.data_dir, self.data[index][\"img\"])\n",
    "        text = self.data[index][\"text\"]\n",
    "        \n",
    "        label = self.data[index][\"label\"]\n",
    "            \n",
    "        return path,  text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0   8291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2   3745  img/03745.png      1   \n",
       "\n",
       "                                                text  \n",
       "0              white people is this a shooting range  \n",
       "1                              bravery at its finest  \n",
       "2  your order comes to $37.50 and your white priv...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'E:\\datasets\\MADE\\3_graduation\\parthplc\\archive\\data\\\\'\n",
    "\n",
    "train_path = data_dir + 'train.jsonl'\n",
    "dev_path = data_dir + 'dev.jsonl'\n",
    "\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(dev_path, lines=True)\n",
    "\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667342583941,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "1s3PMb8Asp8d"
   },
   "outputs": [],
   "source": [
    "train_dataset = HatefulMemesDataset(train_path)\n",
    "val_dataset = HatefulMemesDataset(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667342583941,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "lo7eM5r-tV5W"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5671d8dc55454aab895a3c5df8021d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/08291.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/46971.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/03745.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/83745.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/80243.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/05279.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/01796.png', 'E:\\\\datasets\\\\MADE\\\\3_graduation\\\\parthplc\\\\archive\\\\data\\\\img/53046.png')\n",
      "('white people is this a shooting range', 'bravery at its finest', 'your order comes to $37.50 and your white privilege discount brings the total to $37.50', 'it is time.. to send these parasites back to the desert', 'mississippi wind chime', \"knowing white people , that's probably the baby father\", 'life hack #23 how to get stoned with no weed', \"you've heard of elf on a shelf, now get ready for\")\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paths, texts, labels in tqdm(DataLoader(val_dataset, batch_size=8)):\n",
    "    print(paths)\n",
    "    print(texts)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1667342583941,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "BAjub3VYtiXK"
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1667342595280,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "7-5rqN-vtlkq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1667342611057,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "0ltRR2kDSp83"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH5jfmuH4yuZ"
   },
   "source": [
    "## Tips for putting it all together\n",
    "\n",
    "Note that these methods can be combined into different parts to make it more efficient: \n",
    "1. Get the model and store it in a variable.\n",
    "2. Transform and create batched inputs separately.\n",
    "3. Generate visual embeddings from the detectron on the batched inputs and models.\n",
    "\n",
    "Ideally, you want to build a class around this for ease of use - The class should contain all the methods, the model and the configuration details. And it should process a batch of images and convert to embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLSk85svCziz"
   },
   "source": [
    "## Using the embeddings with VisualBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3313,
     "status": "ok",
     "timestamp": 1667342614799,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "odw_QQo7I0cl",
    "outputId": "62099cde-c5e5-4e45-bd23-4857d07c2677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "# %cd /content/\n",
    "# user = input('User name: ')\n",
    "# password = getpass('Password: ')\n",
    "# password = urllib.parse.quote(password) # your password is converted into url format\n",
    "# cmd_string = f'git clone -b add_visualbert --single-branch https://{user}:{password}@github.com/gchhablani/transformers.git'\n",
    "# os.system(cmd_string)\n",
    "# cmd_string, password = \"\", \"\" # removing the password from the variable\n",
    "# %cd transformers\n",
    "# !pip install -e \".[dev]\"\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667342614799,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "CdivyuVLC6fq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\mmf\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667342614800,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vk2YLidFFHha"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1667342615465,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "-E25fVeCFCz0"
   },
   "outputs": [],
   "source": [
    "questions = [question1]#[question1, question2]\n",
    "tokens = tokenizer(questions, padding='max_length', max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667342615466,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "Rhwgi1H-TU8-",
    "outputId": "80a9146e-0a34-493c-9843-8d84019887b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2054, 2003, 1996, 2611, 3061, 2006, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667342615466,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "tqsPbWONGhTF"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokens[\"input_ids\"])\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "token_type_ids = torch.tensor(tokens[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667342615466,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "mEB4OP33IOCl"
   },
   "outputs": [],
   "source": [
    "visual_embeds = torch.stack(visual_embeds)\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "executionInfo": {
     "elapsed": 1952,
     "status": "ok",
     "timestamp": 1667342617416,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "AR8eDYeCIhxC"
   },
   "outputs": [],
   "source": [
    "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre') # this checkpoint has 1024 dimensional visual embeddings projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667342617416,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "-IWxLhfATG9V",
    "outputId": "9f659fac-5ffc-4d85-c17e-46a1ab61876e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2054, 2003, 1996, 2611, 3061, 2006, 1029,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667342617417,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "Sfl7721RTNI9",
    "outputId": "22b2166f-7cf8-49e6-dc5a-e2f4f72a3ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1667342618155,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "EDdlJxRYIxcT"
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667342618155,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "h-3P82lYNA3y",
    "outputId": "d179fa3b-70b0-4b3e-f857-2be42d742c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualBertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -6.4359,  -6.2968,  -6.5019,  ...,  -6.6693,  -7.2687,  -7.0636],\n",
       "         [ -9.0872,  -8.8805,  -8.9193,  ..., -10.0792,  -8.8213,  -8.9826],\n",
       "         [-10.0700, -10.0092, -10.0334,  ..., -10.7274, -10.3397,  -7.7949],\n",
       "         ...,\n",
       "         [ -6.0317,  -5.9081,  -6.0876,  ...,  -6.1065,  -6.7837,  -7.5046],\n",
       "         [ -6.2400,  -6.2881,  -6.4031,  ...,  -6.6600,  -7.0552,  -7.0159],\n",
       "         [ -5.7199,  -5.8479,  -5.8892,  ...,  -6.4322,  -6.5893,  -7.2381]]],\n",
       "       grad_fn=<ViewBackward0>), seq_relationship_logits=tensor([[0.2899, 0.2922]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667342618155,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "sQ69OfF4SU6I",
    "outputId": "66ff6ff7-6213-4c55-e05c-0e4df03b9e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150, 30522])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1667342633105,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "2fBAKfvpT7_z",
    "outputId": "ae53657b-49b4-4c04-e991-aec602b7c3aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4578300])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.reshape((batch_size,150*30522)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmq8C39meEZX"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Detectron2 Colab Tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)\n",
    "2. [Detectron Repository](https://github.com/facebookresearch/Detectron)\n",
    "3. [Detectron2 Repository](https://github.com/facebookresearch/detectron2)\n",
    "4. [Detectron2 Docs](https://detectron2.readthedocs.io/en/latest/index.html)\n",
    "5. [VisualBert Repository](https://github.com/uclanlp/visualbert)\n",
    "6. [Medium Article on Detectron2 by Hiroto Honda](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
