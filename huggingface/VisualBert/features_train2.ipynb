{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "from transformers import BertTokenizer, VisualBertForPreTraining\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0   8291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2   3745  img/03745.png      1   \n",
       "\n",
       "                                                text  \n",
       "0              white people is this a shooting range  \n",
       "1                              bravery at its finest  \n",
       "2  your order comes to $37.50 and your white priv...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'E:\\datasets\\MADE\\3_graduation\\parthplc\\archive\\data\\\\'\n",
    "\n",
    "train_path = data_dir + 'train.jsonl'\n",
    "dev_path = data_dir + 'dev.jsonl'\n",
    "\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(dev_path, lines=True)\n",
    "\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d:\\\\visual_embeddings_val.pkl', 'rb') as f:\n",
    "    visual_embeddings_val = pickle.load(f)\n",
    "    \n",
    "val_dict = {}\n",
    "for x in test_data.values:\n",
    "    if x[1] in visual_embeddings_val:\n",
    "        val_dict[x[1]] = {'label':x[2], 'text':x[3], 'id':x[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_embeddings_val['img/08291.png'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, visual_embeddings, labels):\n",
    "        self.visual_embeddings = visual_embeddings\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.idx2id = [{'id':k, 'label':labels[k]['label'], 'text':labels[k]['text']}\n",
    "                       for i, k in enumerate(labels)]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        id = self.idx2id[index]['id']\n",
    "        return id, self.visual_embeddings[id][0], self.labels[id]['text'][:77], self.labels[id]['label']\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667342614800,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vk2YLidFFHha"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_val_dataset = FeaturesDataset(visual_embeddings_val, val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.cls.predictions.decoder = torch.nn.Linear(in_features=768, out_features=2, bias=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i_step, (id, visual_embeds, text, y) in enumerate(DataLoader(features_val_dataset, batch_size=1)):\n",
    "    visual_embeds = visual_embeds.to(device)    \n",
    "    tokens = tokenizer(list(text), padding='max_length', max_length=77)\n",
    "\n",
    "    input_ids = torch.tensor(tokens[\"input_ids\"], device=device)\n",
    "    attention_mask = torch.tensor(tokens[\"attention_mask\"], device=device)\n",
    "    token_type_ids = torch.tensor(tokens[\"token_type_ids\"], device=device)\n",
    "\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                      attention_mask=attention_mask, \n",
    "                      token_type_ids=token_type_ids, \n",
    "                      visual_embeds=visual_embeds, \n",
    "                      visual_attention_mask=visual_attention_mask, \n",
    "                      visual_token_type_ids=visual_token_type_ids\n",
    "                  )\n",
    "\n",
    "    prediction = outputs.prediction_logits\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 177, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualBertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ 0.3972, -0.8051],\n",
       "         [-0.3532, -1.6152],\n",
       "         [-0.6077, -0.4447],\n",
       "         [-0.0478, -1.1056],\n",
       "         [ 0.8540, -0.3323],\n",
       "         [-0.8882, -2.1188],\n",
       "         [-0.3672,  0.0482],\n",
       "         [ 0.4527, -0.0895],\n",
       "         [ 0.9576,  1.2770],\n",
       "         [ 1.0655, -0.2704],\n",
       "         [ 1.0107, -0.3919],\n",
       "         [ 0.8331, -0.3004],\n",
       "         [ 0.8630, -0.4667],\n",
       "         [ 0.7233, -0.4353],\n",
       "         [ 0.6346, -0.5200],\n",
       "         [ 0.8346, -0.5998],\n",
       "         [ 0.8405, -0.4731],\n",
       "         [ 0.9949, -0.4285],\n",
       "         [ 1.1617, -0.5671],\n",
       "         [ 0.8890, -0.6670],\n",
       "         [ 0.9999, -0.4627],\n",
       "         [ 0.6701, -0.2886],\n",
       "         [ 0.7732, -0.0410],\n",
       "         [ 0.6819, -0.0751],\n",
       "         [ 0.4768, -0.2325],\n",
       "         [ 0.0448, -0.5686],\n",
       "         [-0.1948, -0.5440],\n",
       "         [-0.1687, -0.4639],\n",
       "         [ 0.1521, -0.3796],\n",
       "         [ 0.6316, -0.1558],\n",
       "         [ 0.8574, -0.1693],\n",
       "         [ 1.0308, -0.1524],\n",
       "         [ 0.6905, -0.2835],\n",
       "         [ 0.8152, -0.3927],\n",
       "         [ 1.0802, -0.6686],\n",
       "         [ 0.6745, -0.2956],\n",
       "         [ 1.0541, -0.2002],\n",
       "         [ 0.8940, -0.3358],\n",
       "         [ 0.7457, -0.1878],\n",
       "         [ 0.6207, -0.2732],\n",
       "         [ 0.4926, -0.2575],\n",
       "         [ 0.5987, -0.3929],\n",
       "         [ 0.4980, -0.3899],\n",
       "         [ 0.7953, -0.2712],\n",
       "         [ 0.9574, -0.2374],\n",
       "         [ 0.9685, -0.4660],\n",
       "         [ 0.7871, -0.4741],\n",
       "         [ 0.7848, -0.7187],\n",
       "         [ 0.9509, -0.6978],\n",
       "         [ 0.5572, -0.1571],\n",
       "         [ 0.6104, -0.1131],\n",
       "         [ 0.3499, -0.3362],\n",
       "         [ 0.3572, -0.0683],\n",
       "         [ 0.0766, -0.3565],\n",
       "         [-0.0701, -0.4453],\n",
       "         [-0.0777, -0.4749],\n",
       "         [-0.0130, -0.3350],\n",
       "         [-0.0902, -0.1894],\n",
       "         [ 1.1055, -0.0721],\n",
       "         [ 1.0304, -0.1244],\n",
       "         [ 0.1874, -0.2147],\n",
       "         [ 0.3766, -0.0586],\n",
       "         [ 0.6499, -0.5005],\n",
       "         [ 0.9365, -0.3898],\n",
       "         [ 0.8161, -0.2212],\n",
       "         [ 0.3974, -0.2125],\n",
       "         [ 0.6551, -0.5303],\n",
       "         [ 0.6796, -0.4165],\n",
       "         [ 0.3415, -0.1795],\n",
       "         [ 0.5704, -0.3978],\n",
       "         [ 0.3879, -0.3391],\n",
       "         [ 0.5933, -0.2387],\n",
       "         [ 0.5864, -0.2655],\n",
       "         [ 0.8882, -0.5377],\n",
       "         [ 0.5602, -0.4111],\n",
       "         [ 0.9435, -0.9215],\n",
       "         [ 0.6525, -0.4455],\n",
       "         [ 0.3235, -0.9923],\n",
       "         [ 0.3225, -0.9503],\n",
       "         [ 0.4801, -0.8888],\n",
       "         [ 0.4852, -0.8339],\n",
       "         [ 0.4763, -0.8163],\n",
       "         [ 0.4453, -0.8608],\n",
       "         [ 0.4415, -0.9255],\n",
       "         [ 0.3593, -0.9532],\n",
       "         [ 0.3646, -0.9356],\n",
       "         [ 0.3390, -0.9536],\n",
       "         [ 0.3616, -0.9510],\n",
       "         [ 0.4256, -0.8937],\n",
       "         [ 0.3626, -0.9385],\n",
       "         [ 0.3594, -0.9478],\n",
       "         [ 0.3624, -0.9377],\n",
       "         [ 0.4068, -0.9110],\n",
       "         [ 0.3678, -0.9472],\n",
       "         [ 0.3493, -0.9486],\n",
       "         [ 0.3615, -0.9635],\n",
       "         [ 0.3622, -0.9356],\n",
       "         [ 0.3716, -0.9434],\n",
       "         [ 0.5683, -0.6569],\n",
       "         [ 0.3407, -0.9918],\n",
       "         [ 0.3502, -0.9683],\n",
       "         [ 0.3422, -0.9778],\n",
       "         [ 0.3640, -0.9512],\n",
       "         [ 0.3177, -0.7219],\n",
       "         [ 0.3642, -0.6854],\n",
       "         [ 0.3227, -0.9287],\n",
       "         [ 0.4225, -0.9419],\n",
       "         [ 0.3244, -0.7949],\n",
       "         [ 0.3070, -0.9621],\n",
       "         [ 0.3069, -0.9622],\n",
       "         [ 0.3217, -0.9595],\n",
       "         [ 0.5168, -0.6778],\n",
       "         [ 0.2950, -0.7235],\n",
       "         [ 0.3223, -0.9507],\n",
       "         [ 0.3039, -0.9805],\n",
       "         [ 0.2690, -0.8607],\n",
       "         [ 0.5328, -0.8186],\n",
       "         [ 0.3115, -0.9034],\n",
       "         [ 0.5365, -0.7692],\n",
       "         [ 0.5224, -0.8592],\n",
       "         [ 0.5342, -0.7573],\n",
       "         [ 0.3099, -0.9151],\n",
       "         [ 0.5391, -0.7866],\n",
       "         [ 0.5184, -0.8511],\n",
       "         [ 0.3125, -0.8793],\n",
       "         [ 0.5327, -1.0239],\n",
       "         [ 0.4645, -0.9114],\n",
       "         [ 0.5849, -0.9989],\n",
       "         [ 0.5940, -0.9930],\n",
       "         [ 0.5504, -1.0261],\n",
       "         [ 0.3106, -0.8835],\n",
       "         [ 0.3064, -0.8891],\n",
       "         [ 0.3053, -0.9321],\n",
       "         [ 0.1777, -0.7864],\n",
       "         [ 0.3033, -0.8903],\n",
       "         [ 0.3057, -0.8853],\n",
       "         [ 0.6152, -0.6817],\n",
       "         [ 0.3093, -0.9205],\n",
       "         [ 0.5710, -1.0648],\n",
       "         [ 0.2857, -0.9075],\n",
       "         [ 0.5586, -1.0066],\n",
       "         [ 0.3890, -1.0062],\n",
       "         [ 0.5304, -1.0440],\n",
       "         [ 0.4474, -0.8577],\n",
       "         [ 0.4937, -0.8812],\n",
       "         [ 0.3822, -0.9046],\n",
       "         [ 0.3626, -1.0118],\n",
       "         [ 0.4222, -0.9368],\n",
       "         [ 0.4252, -0.9385],\n",
       "         [ 0.3315, -0.9872],\n",
       "         [ 0.4172, -1.0334],\n",
       "         [ 0.5948, -0.7327],\n",
       "         [ 0.5042, -0.8582],\n",
       "         [ 0.4821, -0.8871],\n",
       "         [ 0.3963, -1.0332],\n",
       "         [ 0.4861, -0.8914],\n",
       "         [ 0.2783, -0.9057],\n",
       "         [ 0.4224, -1.0655],\n",
       "         [ 0.4715, -0.7928],\n",
       "         [ 0.4473, -1.1240],\n",
       "         [ 0.4809, -0.8904],\n",
       "         [ 0.4593, -0.8096],\n",
       "         [ 0.5551, -0.7128],\n",
       "         [ 0.4364, -1.0753],\n",
       "         [ 0.5797, -0.7715],\n",
       "         [ 0.5046, -0.8478],\n",
       "         [ 0.5216, -0.7960],\n",
       "         [ 0.5083, -0.8460],\n",
       "         [ 0.5067, -0.7617],\n",
       "         [ 0.5054, -0.8242],\n",
       "         [ 0.4934, -1.1633],\n",
       "         [ 0.4959, -0.8355],\n",
       "         [ 0.5023, -0.8239],\n",
       "         [ 0.4604, -0.7642],\n",
       "         [ 0.5078, -0.8209],\n",
       "         [ 0.5070, -0.8205],\n",
       "         [ 0.5888, -0.7022]]], device='cuda:0', grad_fn=<AddBackward0>), seq_relationship_logits=tensor([[0.9139, 0.0146]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  85.9483, -118.9581]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
