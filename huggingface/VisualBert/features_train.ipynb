{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "from transformers import BertTokenizer, VisualBertForPreTraining\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white priv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0   8291  img/08291.png      1   \n",
       "1  46971  img/46971.png      1   \n",
       "2   3745  img/03745.png      1   \n",
       "\n",
       "                                                text  \n",
       "0              white people is this a shooting range  \n",
       "1                              bravery at its finest  \n",
       "2  your order comes to $37.50 and your white priv...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'E:\\datasets\\MADE\\3_graduation\\parthplc\\archive\\data\\\\'\n",
    "\n",
    "train_path = data_dir + 'train.jsonl'\n",
    "dev_path = data_dir + 'dev.jsonl'\n",
    "\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(dev_path, lines=True)\n",
    "\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d:\\\\visual_embeddings_val.pkl', 'rb') as f:\n",
    "    visual_embeddings_val = pickle.load(f)\n",
    "    \n",
    "val_dict = {}\n",
    "for x in test_data.values:\n",
    "    if x[1] in visual_embeddings_val:\n",
    "        val_dict[x[1]] = {'label':x[2], 'text':x[3], 'id':x[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d:\\\\visual_embeddings_train.pkl', 'rb') as f:\n",
    "    visual_embeddings_train = pickle.load(f)\n",
    "    \n",
    "train_dict = {}\n",
    "for x in train_data.values:\n",
    "    if x[1] in visual_embeddings_train:\n",
    "        train_dict[x[1]] = {'label':x[2], 'text':x[3], 'id':x[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, visual_embeddings, labels):\n",
    "        self.visual_embeddings = visual_embeddings\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.idx2id = [{'id':k, 'label':labels[k]['label'], 'text':labels[k]['text']}\n",
    "                       for i, k in enumerate(labels)]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        id = self.idx2id[index]['id']\n",
    "        return id, self.visual_embeddings[id][0], self.labels[id]['text'][:77], self.labels[id]['label']\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, device):\n",
    "    best_model_name = None\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    top_val_accuracy = 0.64 \n",
    "    for epoch in range(num_epochs):\n",
    "        t1 = time.time()\n",
    "        model.train()\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (id, visual_embeds, text, y) in enumerate(train_loader):\n",
    "            y = y.to(device)\n",
    "            visual_embeds = visual_embeds.to(device)    \n",
    "            tokens = tokenizer(list(text), padding='max_length', max_length=77)\n",
    "\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"], device=device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"], device=device)\n",
    "            token_type_ids = torch.tensor(tokens[\"token_type_ids\"], device=device)\n",
    "\n",
    "            visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "            visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask, \n",
    "                              token_type_ids=token_type_ids, \n",
    "                              visual_embeds=visual_embeds, \n",
    "                              visual_attention_mask=visual_attention_mask, \n",
    "                              visual_token_type_ids=visual_token_type_ids\n",
    "                          )\n",
    "    \n",
    "            prediction = outputs.prediction_logits.sum(axis=1)\n",
    "            \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader, device)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(\"Epoch: %i; %.2f sec; lr: %f; Average loss: %.2f, Train accuracy: %.4f, Val accuracy: %.4f\" % \n",
    "              (epoch, round(time.time() - t1, 2), get_lr(optimizer), ave_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "  \n",
    "        if val_accuracy > top_val_accuracy:\n",
    "            top_val_accuracy = val_accuracy\n",
    "            model_name = f'classifier_{epoch}_{round(val_accuracy, 3)}.ckpt'\n",
    "            best_model_name = model_name\n",
    "            torch.save(model, open(model_name, 'wb'))\n",
    "            print(\"saved\", model_name)\n",
    "\n",
    "        if len(val_history) > 4:\n",
    "            print(f'{(val_history[-1] - val_history[-2]) < 0.001} {(val_history[-2] - val_history[-3]) < 0.001} \\\n",
    "            {(val_history[-3] - val_history[-4]) < 0.001} {(val_history[-4] - val_history[-5]) < 0.001}')\n",
    "        \n",
    "        if len(val_history) > 4 and (val_history[-1] - val_history[-2]) < 0.001  and \\\n",
    "                                    (val_history[-2] - val_history[-3]) < 0.001 and \\\n",
    "                                    (val_history[-3] - val_history[-4]) < 0.001 and \\\n",
    "                                    (val_history[-4] - val_history[-5]) < 0.001:\n",
    "            print('pruned')\n",
    "            return loss_history, train_history, val_history, best_model_name\n",
    "        \n",
    "    return loss_history, train_history, val_history, best_model_name\n",
    "        \n",
    "    \n",
    "def compute_accuracy(model, loader, device):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_samples = 0\n",
    "    total_samples = 0 \n",
    "    for i_step, (id, visual_embeds, text, y) in enumerate(loader):\n",
    "        y = y.to(device)\n",
    "        visual_embeds = visual_embeds.to(device)    \n",
    "        tokens = tokenizer(list(text), padding='max_length', max_length=77)\n",
    "\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"], device=device)\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"], device=device)\n",
    "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"], device=device)\n",
    "\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask, \n",
    "                          token_type_ids=token_type_ids, \n",
    "                          visual_embeds=visual_embeds, \n",
    "                          visual_attention_mask=visual_attention_mask, \n",
    "                          visual_token_type_ids=visual_token_type_ids\n",
    "                      )\n",
    "    \n",
    "        prediction = outputs.prediction_logits.sum(axis=1)\n",
    "            \n",
    "        _, indices = torch.max(prediction, 1)\n",
    "        correct_samples += torch.sum(indices == y)\n",
    "        total_samples += y.shape[0]            \n",
    "\n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "         \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667342614800,
     "user": {
      "displayName": "Шишкин Александр",
      "userId": "00721730077610513143"
     },
     "user_tz": -240
    },
    "id": "vk2YLidFFHha"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_dataset = FeaturesDataset(visual_embeddings_train, train_dict)\n",
    "features_val_dataset = FeaturesDataset(visual_embeddings_val, val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value: 0.5431559571619813\n",
    "#Parameters: {'layer_count': 207, 'step_size': 4, 'batch_size': 544, 'learning_rate': 0.009455928480805944, 'gamma': 0.5861260817780743}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 cls.predictions.decoder.weight\n",
      "209 cls.predictions.decoder.bias\n",
      "211 cls.seq_relationship.bias\n",
      "Epoch: 0; 34.74 sec; lr: 0.009456; Average loss: 257.74, Train accuracy: 0.5591, Val accuracy: 0.5040\n",
      "Epoch: 1; 68.05 sec; lr: 0.009456; Average loss: 124.25, Train accuracy: 0.5402, Val accuracy: 0.5000\n",
      "Epoch: 2; 101.40 sec; lr: 0.009456; Average loss: 84.75, Train accuracy: 0.5616, Val accuracy: 0.5382\n",
      "Epoch: 3; 134.80 sec; lr: 0.005542; Average loss: 88.15, Train accuracy: 0.5737, Val accuracy: 0.5040\n",
      "Epoch: 4; 168.34 sec; lr: 0.005542; Average loss: 72.43, Train accuracy: 0.5534, Val accuracy: 0.5141\n",
      "False True             False True\n",
      "Epoch: 5; 201.82 sec; lr: 0.005542; Average loss: 27.53, Train accuracy: 0.6016, Val accuracy: 0.5281\n",
      "False False             True False\n",
      "Epoch: 6; 235.40 sec; lr: 0.005542; Average loss: 16.95, Train accuracy: 0.6098, Val accuracy: 0.5100\n",
      "True False             False True\n",
      "Epoch: 7; 268.97 sec; lr: 0.003249; Average loss: 42.00, Train accuracy: 0.5514, Val accuracy: 0.5241\n",
      "False True             False False\n",
      "Epoch: 8; 302.53 sec; lr: 0.003249; Average loss: 15.60, Train accuracy: 0.5930, Val accuracy: 0.5000\n",
      "True False             True False\n",
      "Epoch: 9; 336.13 sec; lr: 0.003249; Average loss: 30.53, Train accuracy: 0.5691, Val accuracy: 0.5100\n",
      "False True             False True\n",
      "Epoch: 10; 369.64 sec; lr: 0.003249; Average loss: 36.11, Train accuracy: 0.5551, Val accuracy: 0.4960\n",
      "True False             True False\n",
      "Epoch: 11; 403.16 sec; lr: 0.001904; Average loss: 42.09, Train accuracy: 0.5262, Val accuracy: 0.5482\n",
      "False True             False True\n",
      "Epoch: 12; 436.75 sec; lr: 0.001904; Average loss: 19.94, Train accuracy: 0.5634, Val accuracy: 0.5080\n",
      "True False             True False\n",
      "Epoch: 13; 470.24 sec; lr: 0.001904; Average loss: 11.79, Train accuracy: 0.6138, Val accuracy: 0.5261\n",
      "False True             False True\n",
      "Epoch: 14; 503.85 sec; lr: 0.001904; Average loss: 16.67, Train accuracy: 0.5854, Val accuracy: 0.5201\n",
      "True False             True False\n",
      "Epoch: 15; 537.36 sec; lr: 0.001116; Average loss: 17.40, Train accuracy: 0.5774, Val accuracy: 0.5120\n",
      "True True             False True\n",
      "Epoch: 16; 570.94 sec; lr: 0.001116; Average loss: 17.48, Train accuracy: 0.5718, Val accuracy: 0.5241\n",
      "False True             True False\n",
      "Epoch: 17; 604.56 sec; lr: 0.001116; Average loss: 11.64, Train accuracy: 0.6027, Val accuracy: 0.5382\n",
      "False False             True True\n",
      "Epoch: 18; 638.14 sec; lr: 0.001116; Average loss: 11.20, Train accuracy: 0.5806, Val accuracy: 0.5261\n",
      "True False             False True\n",
      "Epoch: 19; 671.71 sec; lr: 0.000654; Average loss: 10.33, Train accuracy: 0.5907, Val accuracy: 0.5341\n",
      "False True             False False\n",
      "Epoch: 20; 705.36 sec; lr: 0.000654; Average loss: 10.76, Train accuracy: 0.5865, Val accuracy: 0.5301\n",
      "True False             True False\n",
      "Epoch: 21; 738.92 sec; lr: 0.000654; Average loss: 9.93, Train accuracy: 0.5942, Val accuracy: 0.5040\n",
      "True True             False True\n",
      "Epoch: 22; 772.57 sec; lr: 0.000654; Average loss: 8.92, Train accuracy: 0.5938, Val accuracy: 0.5161\n",
      "False True             True False\n",
      "Epoch: 23; 806.15 sec; lr: 0.000383; Average loss: 7.75, Train accuracy: 0.6059, Val accuracy: 0.5100\n",
      "True False             True True\n",
      "Epoch: 24; 839.74 sec; lr: 0.000383; Average loss: 7.75, Train accuracy: 0.6054, Val accuracy: 0.5120\n",
      "False True             False True\n",
      "Epoch: 25; 873.40 sec; lr: 0.000383; Average loss: 7.99, Train accuracy: 0.5964, Val accuracy: 0.5221\n",
      "False False             True False\n",
      "Epoch: 26; 906.99 sec; lr: 0.000383; Average loss: 7.82, Train accuracy: 0.5922, Val accuracy: 0.5221\n",
      "True False             False True\n",
      "Epoch: 27; 940.65 sec; lr: 0.000225; Average loss: 8.04, Train accuracy: 0.5890, Val accuracy: 0.5281\n",
      "False True             False False\n",
      "Epoch: 28; 974.23 sec; lr: 0.000225; Average loss: 7.61, Train accuracy: 0.5885, Val accuracy: 0.5080\n",
      "True False             True False\n",
      "Epoch: 29; 1007.82 sec; lr: 0.000225; Average loss: 6.69, Train accuracy: 0.6137, Val accuracy: 0.5120\n",
      "False True             False True\n"
     ]
    }
   ],
   "source": [
    "layer_count=207\n",
    "step_size=4\n",
    "batch_size=544\n",
    "learning_rate=0.009455928480805944\n",
    "gamma=0.5861260817780743\n",
    "\n",
    "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False\n",
    "    if i > layer_count:\n",
    "        break\n",
    "\n",
    "model.cls.predictions.decoder = torch.nn.Linear(in_features=768, out_features=2, bias=True)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params.append(param)        \n",
    "\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad == True:\n",
    "        print(i, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i_step, (id, visual_embeds, text, y) in enumerate(DataLoader(features_val_dataset, batch_size=3)):\n",
    "    visual_embeds = visual_embeds.to(device)    \n",
    "    tokens = tokenizer(list(text), padding='max_length', max_length=77)\n",
    "\n",
    "    input_ids = torch.tensor(tokens[\"input_ids\"], device=device)\n",
    "    attention_mask = torch.tensor(tokens[\"attention_mask\"], device=device)\n",
    "    token_type_ids = torch.tensor(tokens[\"token_type_ids\"], device=device)\n",
    "\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                      attention_mask=attention_mask, \n",
    "                      token_type_ids=token_type_ids, \n",
    "                      visual_embeds=visual_embeds, \n",
    "                      visual_attention_mask=visual_attention_mask, \n",
    "                      visual_token_type_ids=visual_token_type_ids\n",
    "                  )\n",
    "\n",
    "    prediction = outputs.prediction_logits\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 177, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_val_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_history, label='loss')\n",
    "plt.plot(train_history, label='train accuracy')\n",
    "plt.plot(val_history, label='val accuracy')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best model:\", best_model_name)\n",
    "\n",
    "best_model = torch.load(open(best_model_name, 'rb'))\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "for i_step, (x, y) in enumerate(DataLoader(features_val_dataset, batch_size=5000)):\n",
    "    prediction = best_model(x)\n",
    "\n",
    "acc_score = accuracy_score(np.array([x.item() for x in labels_val]), torch.max(prediction, 1)[1])\n",
    "auc_score = roc_auc_score(np.array([x.item() for x in labels_val]), prediction[:,1].detach().numpy())\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(labels_val, prediction[:,1].detach().numpy(), pos_label=1)\n",
    "\n",
    "random_probs = [0 for i in range(len(labels_val))]\n",
    "p_fpr, p_tpr, _ = roc_curve(labels_val, random_probs, pos_label=1)\n",
    "auc_score = roc_auc_score(labels_val, prediction[:,1].detach().numpy())\n",
    "\n",
    "print('Accuracy: ', acc_score, '\\n', 'ROC AUC: ', auc_score, sep='')\n",
    "\n",
    "plt.plot(fpr, tpr, linestyle='--',color='orange')\n",
    "plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive rate',fontsize=18)\n",
    "\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
