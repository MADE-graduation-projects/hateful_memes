{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "\n",
    "import json\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from CLIP_utils import get_features, get_lr\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyTable = PrettyTable(['Model name', 'Accuracy', 'ROC AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.transforms.CropCenter import CropCenter\n",
    "from classes.transforms.ScaleMaxSideToSize import ScaleMaxSideToSize\n",
    "from classes.dataset.HatefulMemesDataset import HatefulMemesDataset\n",
    "from classes.dataset.FeaturesDataset import FeaturesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'E:\\datasets\\MADE\\3_graduation\\parthplc\\archive\\data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir + 'train.jsonl'\n",
    "dev_path = data_dir + 'dev.jsonl'\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(dev_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "STD = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, model_name):    \n",
    "    best_model_name = None\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    top_val_accuracy = 0.3\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)#x.type(torch.float).cpu()\n",
    "            y = y.to(device)#y.type(torch.float).cpu()\n",
    "            #model = model.cpu()\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y.type(torch.long))\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        #print(\"Epoch: %i lr: %f; Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (epoch, get_lr(optimizer), ave_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "  \n",
    "        if val_accuracy > top_val_accuracy:\n",
    "            #\n",
    "            top_val_accuracy = val_accuracy\n",
    "            m_name = f'{model_name}_classifier_{epoch}_{round(val_accuracy, 3)}.ckpt'\n",
    "            best_model_name = m_name\n",
    "            torch.save(model, open(m_name, 'wb'))\n",
    "            #print(\"saved\", m_name)\n",
    "\n",
    "    return loss_history, train_history, val_history, best_model_name\n",
    "        \n",
    "    \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_samples = 0\n",
    "    total_samples = 0 \n",
    "    for i_step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = model(x)\n",
    "        _, indices = torch.max(prediction, 1)\n",
    "        correct_samples += torch.sum(indices == y)\n",
    "        total_samples += y.shape[0]            \n",
    "\n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "         \n",
    "    return val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(model_name):\n",
    "    CROP_SIZE=224\n",
    "    \n",
    "    if '336' in model_name:\n",
    "        CROP_SIZE=336\n",
    "        \n",
    "    transforms = T.Compose([\n",
    "        ScaleMaxSideToSize(CROP_SIZE),\n",
    "        CropCenter(CROP_SIZE),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD),\n",
    "    ])    \n",
    "    \n",
    "    train_dataset = HatefulMemesDataset(train_path, transforms)\n",
    "    val_dataset = HatefulMemesDataset(dev_path, transforms)\n",
    "\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    features_train, labels_train = get_features(model, train_dataset)\n",
    "    features_val, labels_val = get_features(model, val_dataset)\n",
    "\n",
    "    features_train_dataset = FeaturesDataset(features_train, labels_train)\n",
    "    features_val_dataset = FeaturesDataset(features_val, labels_val)\n",
    "\n",
    "    input_shape = features_train[0].shape[0]\n",
    "    num_classes = 2\n",
    "\n",
    "\n",
    "    torch.manual_seed(1024)\n",
    "\n",
    "    shape = 256\n",
    "    nn_model = nn.Sequential(\n",
    "                nn.Linear(input_shape, shape),\n",
    "                nn.Dropout(0.66),\n",
    "                nn.BatchNorm1d(shape),\n",
    "                nn.ReLU(inplace=True),    \n",
    "\n",
    "                nn.Linear(shape, shape),\n",
    "                nn.Dropout(0.66),\n",
    "                nn.BatchNorm1d(shape),\n",
    "                nn.ReLU(inplace=True),    \n",
    "\n",
    "                nn.Linear(shape, num_classes),\n",
    "                )\n",
    "\n",
    "    nn_model = nn_model.to(device)\n",
    "    #print(nn_model)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "    loss_history, train_history, val_history, best_model_name = train_model(\n",
    "        nn_model, \n",
    "        DataLoader(features_train_dataset, batch_size=500),\n",
    "        DataLoader(features_val_dataset, batch_size=500),\n",
    "        loss, optimizer, 100, scheduler, 'clip')\n",
    "\n",
    "    print(\"best model:\", best_model_name)\n",
    "    best_model = torch.load(open(best_model_name, 'rb'))\n",
    "    #print(best_model)\n",
    "\n",
    "    best_model = best_model.to(device)\n",
    "    \n",
    "    best_model.eval()\n",
    "    for i_step, (x, y) in enumerate(DataLoader(features_val_dataset, batch_size=5000)):\n",
    "        x = x.to(device)\n",
    "        prediction = best_model(x)\n",
    "\n",
    "    acc_score = accuracy_score(np.array([x.item() for x in labels_val]), torch.max(prediction.cpu(), 1)[1])\n",
    "    auc_score = roc_auc_score(np.array([x.item() for x in labels_val]), prediction.cpu()[:,1].detach().numpy())\n",
    "    prettyTable.add_row([model_name, acc_score, auc_score])\n",
    "    print(model_name, CROP_SIZE, input_shape, acc_score, auc_score)\n",
    "\n",
    "    return (acc_score, auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [01:17<00:00,  1.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_87_0.636.ckpt\n",
      "RN50 224 2048 0.636 0.676096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [01:21<00:00,  1.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_74_0.68.ckpt\n",
      "RN101 224 1024 0.68 0.7092319999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/85 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (50) must match the size of tensor b (82) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/85 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (50) must match the size of tensor b (145) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/85 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (50) must match the size of tensor b (197) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [01:14<00:00,  1.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_44_0.654.ckpt\n",
      "ViT-B/32 224 1024 0.654 0.698512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [01:29<00:00,  1.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_93_0.672.ckpt\n",
      "ViT-B/16 224 1024 0.672 0.734128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [02:33<00:00,  1.80s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_49_0.724.ckpt\n",
      "ViT-L/14 224 1536 0.724 0.786672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 85/85 [04:46<00:00,  3.36s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: clip_classifier_14_0.742.ckpt\n",
      "ViT-L/14@336px 336 1536 0.742 0.7866240000000001\n"
     ]
    }
   ],
   "source": [
    "for x in model_names:\n",
    "    try:\n",
    "        results[x] = work(x)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+--------------------+\n",
      "|   Model name   | Accuracy |      ROC AUC       |\n",
      "+----------------+----------+--------------------+\n",
      "|      RN50      |  0.636   |      0.676096      |\n",
      "|     RN101      |   0.68   | 0.7092319999999999 |\n",
      "|    ViT-B/32    |  0.654   |      0.698512      |\n",
      "|    ViT-B/16    |  0.672   |      0.734128      |\n",
      "|    ViT-L/14    |  0.724   |      0.786672      |\n",
      "| ViT-L/14@336px |  0.742   | 0.7866240000000001 |\n",
      "+----------------+----------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "print(prettyTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RN50': (0.636, 0.676096),\n",
       " 'RN101': (0.68, 0.7092319999999999),\n",
       " 'ViT-B/32': (0.654, 0.698512),\n",
       " 'ViT-B/16': (0.672, 0.734128),\n",
       " 'ViT-L/14': (0.724, 0.786672),\n",
       " 'ViT-L/14@336px': (0.742, 0.7866240000000001)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
